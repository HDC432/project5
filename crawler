#!/usr/bin/env python3

import argparse
import socket
import ssl
import re

from HttpModule import HttpCrawler

DEFAULT_SERVER = "fakebook.khoury.northeastern.edu"
DEFAULT_PORT = 443

# Class to complete login, 
class Crawler:
    # Initialize Crawler with server, port, username, password and cookies map
    def __init__(self, args):
        self.server = args.server if args.server else DEFAULT_SERVER
        self.port = args.port if args.port else DEFAULT_PORT
        self.username = args.username
        self.password = args.password
        self.cookies = {}


    # Funtion to extract cookie and store it in self.cookies
    def handle_cookie(self, headers_text):
        cookie_headers = [line for line in headers_text.splitlines() if line.lower().startswith("set-cookie:")]
        for header in cookie_headers:
            cookie_str = header.split(':', 1)[1].strip()
            cookie_parts = cookie_str.split(';')[0].strip().split('=', 1)
            if len(cookie_parts) == 2:
                name, value = cookie_parts
                self.cookies[name] = value


    # Function to send an HTTP request, process responses and handle cookies
    def send_request(self, request):
        # Create a socket connection
        new_socket = socket.create_connection((self.server, self.port))
        ssl_context = ssl.create_default_context()
        mysocket = ssl_context.wrap_socket(new_socket, server_hostname=self.server)
        mysocket.send(request.encode('ascii'))

        # Decode response
        response_data = b""
        while True:
            data = mysocket.recv(4096)
            if not data:
                break
            response_data += data
        mysocket.close()
        response_text = response_data.decode('ascii', errors='ignore')

        # Process cookie from response
        self.handle_cookie(response_text)

        return response_text


    # Function to get csrf token to ensure login successfully
    def get_csrf_token(self):
        login_page_request = f"GET /accounts/login/?next=/fakebook/ HTTP/1.1\r\nHost: {self.server}\r\nUser-Agent: Crawler/1.0\r\nConnection: close\r\n\r\n"
        response_text = self.send_request(login_page_request)

        # Search for CSRF cookie by regex
        csrf_token_match = re.search(r'name="csrfmiddlewaretoken" value="(.+?)"', response_text)
        csrf_token = csrf_token_match.group(1) if csrf_token_match else None

        # Warning message if no CSRF token is found
        if not csrf_token:
            print("Warning: CSRF token not found in response.")
            return None

        return csrf_token


    # Function to login
    def login(self):
        csrf_token = self.get_csrf_token()
        if not csrf_token:
            print("Failed to retrieve CSRF token.")
            return False

        if 'csrftoken' not in self.cookies:
            print("Missing csrftoken cookie.")
            return False

        # Make login data and HTTP request headers
        login_data = (
            f"username={self.username}"
            f"&password={self.password}"
            f"&csrfmiddlewaretoken={csrf_token}"
            f"&next=/fakebook/"
        )

        headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Referer": f"https://{self.server}/accounts/login/?next=/fakebook/",
            "Cookie": "; ".join([f"{k}={v}" for k, v in self.cookies.items()]),
            "User-Agent": "Crawler/1.0",
            "Host": self.server,
            "Connection": "keep-alive"
        }

        login_request = f"POST /accounts/login/ HTTP/1.1\r\nHost: {self.server}\r\n"
        for header, value in headers.items():
            login_request += f"{header}: {value}\r\n"
        login_request += f"Content-Length: {len(login_data)}\r\n"
        login_request += "Connection: close\r\n\r\n"
        login_request += login_data

        response_text = self.send_request(login_request)

        # Check seesion id to ensure login successfully
        if "sessionid" in response_text:
            return True
        else:
            print("Login failed.")
            return False


    # Run the login and crawling program
    def run(self):
        if not self.login():
            print("Exiting due to login failure.")
            return
        # Initialize and start HTTPCrawler
        httpCrawler = HttpCrawler(self.server, self.port, self.cookies)
        httpCrawler.crawl()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
